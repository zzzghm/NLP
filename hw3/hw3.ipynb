{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this assignment, fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`, as well as your name below.\n",
    "\n",
    "To make sure everything runs as expected, do the following\n",
    "- **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart)\n",
    "- **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "A good introduction to Jupyter notebooks is [here](https://realpython.com/jupyter-notebook-introduction/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Joy Zhuge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Data-structures\" data-toc-modified-id=\"Data-structures-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data structures</a></span></li><li><span><a href=\"#Parser-states-and-actions\" data-toc-modified-id=\"Parser-states-and-actions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Parser states and actions</a></span></li><li><span><a href=\"#Training-an-action-classifier\" data-toc-modified-id=\"Training-an-action-classifier-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training an action classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-training-data\" data-toc-modified-id=\"Generate-training-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Generate training data</a></span></li><li><span><a href=\"#Compute-features\" data-toc-modified-id=\"Compute-features-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Compute features</a></span></li><li><span><a href=\"#Fit-classifier\" data-toc-modified-id=\"Fit-classifier-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Fit classifier</a></span></li></ul></li><li><span><a href=\"#Using-the-classifier-to-parse\" data-toc-modified-id=\"Using-the-classifier-to-parse-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Using the classifier to parse</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Bonus:-Feature-engineering\" data-toc-modified-id=\"Bonus:-Feature-engineering-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Bonus: Feature engineering</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e686116302ae1d3b0e1d1cb265a4a9c",
     "grade": false,
     "grade_id": "jupyter",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "In this assignment we will implement a transition-based dependency parser, using machine learning to choose the best transition at each step.\n",
    "\n",
    "As in the previous assignment, there are spaces below for you to both write code and short answers. In some places, there are tests to check your work, though passing tests does not guarantee full credit. I recommend moving sequentially from top to bottom, getting each step working before moving on to the next.\n",
    "\n",
    "This assignment will use a number of Python libraries, listed in the next cell. If you haven't already installed these, you can do so by running this command in this directory: `pip install -r requirements.txt`. Minor variants in the version numbers shouldn't affect things much.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter, defaultdict, deque\n",
    "import numpy as np\n",
    "from numpy import array as npa\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures\n",
    "\n",
    "First, we need a data structure to store dependency trees. We'll use the one below. For simplicity, we'll assume unlabeled dependency trees.\n",
    "\n",
    "Note that we need the word position in the sentence to deal with the case where a word appears more than once.\n",
    "\n",
    "The children are stored as a list of other DTree objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTree:\n",
    "    def __init__(self, word, position):\n",
    "        self.word = word\n",
    "        self.position = position\n",
    "        self.children = []\n",
    "        \n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # for pretty printing a single node\n",
    "        return '%s-%d' % (self.word, self.position)\n",
    "    \n",
    "    def print(self, indent=0):\n",
    "        # for printing a node and all its descendents\n",
    "        print(' '*indent + '-'*min(2, indent) + str(self))\n",
    "        for c in self.children:\n",
    "            c.print(indent + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example tree for our example \"Book me the morning flight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT-0\n",
      "  --Book-1\n",
      "    --me-2\n",
      "    --flight-5\n",
      "      --the-3\n",
      "      --morning-4\n"
     ]
    }
   ],
   "source": [
    "def example_tree():\n",
    "    root = DTree('ROOT', 0)\n",
    "    book = DTree('Book', 1)\n",
    "    me = DTree('me', 2)\n",
    "    the = DTree('the', 3)\n",
    "    morning = DTree('morning', 4)\n",
    "    flight = DTree('flight', 5)\n",
    "\n",
    "    root.add_child(book)\n",
    "    book.add_child(me)\n",
    "    book.add_child(flight)\n",
    "    flight.add_child(the)\n",
    "    flight.add_child(morning)\n",
    "    return root\n",
    "\n",
    "root = example_tree()\n",
    "root.print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "    \n",
    "```\n",
    "ROOT-0\n",
    "  --Book-1\n",
    "    --me-2\n",
    "    --flight-5\n",
    "      --the-3\n",
    "      --morning-4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a `DTree` defined above, we will often want to extract the parents of each word. Complete the method below, which takes in a DTree and returns a `dict` that maps each `DTree` node to its parent node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf54de3f3dc725ef4b66bb7087453afb",
     "grade": false,
     "grade_id": "get_parents",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Book-1': 'ROOT-0',\n",
       " 'me-2': 'Book-1',\n",
       " 'flight-5': 'Book-1',\n",
       " 'the-3': 'flight-5',\n",
       " 'morning-4': 'flight-5'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_parents(node, node2parent):\n",
    "    \"\"\"\n",
    "    Create a dict of string->string indicating who the parent of each node is in \n",
    "    a dependency tree.\n",
    "    \n",
    "    Args:\n",
    "      node..........a DTree node\n",
    "      node2parent...a dict from str to str, where the value is the parent of the key.\n",
    "      You can use str(node) to get the string representation of each node (word-position),\n",
    "      which should be used for both keys and values\n",
    "      \n",
    "    Returns:\n",
    "      nothing. node2parent is passed by reference and is used as the result, to make it \n",
    "      easier to implement this recursively.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    len_c=len(node.children)\n",
    "    for i in range(len_c):\n",
    "        chil=node.children[i]\n",
    "        node2parent[str(chil)]=str(node)\n",
    "        get_parents(chil,node2parent)\n",
    "#     raise NotImplementedError()\n",
    "    return node2parent\n",
    "\n",
    "node2parent = {}\n",
    "get_parents(example_tree(), node2parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "{'Book-1': 'ROOT-0',\n",
    " 'me-2': 'Book-1',\n",
    " 'flight-5': 'Book-1',\n",
    " 'the-3': 'flight-5',\n",
    " 'morning-4': 'flight-5'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "438f8759b4dc24c3787944150226feb8",
     "grade": true,
     "grade_id": "get_parents_test",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test tree: ROOT -> c1 -> c2\n",
    "root = DTree('ROOT', 0)\n",
    "c1 = DTree('C1', 1)\n",
    "c2 = DTree('C2', 2)\n",
    "root.add_child(c1)\n",
    "c1.add_child(c2)\n",
    "\n",
    "node2parent_test = {}\n",
    "parents_test = get_parents(root, node2parent)\n",
    "\n",
    "assert parents_test[str(c1)] == str(root)\n",
    "assert parents_test[str(c2)] == str(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new version\n",
    "# test tree: ROOT -> c1 -> c2\n",
    "root = DTree('ROOT', 0)\n",
    "c1 = DTree('C1', 1)\n",
    "c2 = DTree('C2', 2)\n",
    "root.add_child(c1)\n",
    "c1.add_child(c2)\n",
    "\n",
    "node2parent_test = {}\n",
    "get_parents(root, node2parent_test)\n",
    "\n",
    "assert node2parent_test[str(c1)] == str(root)\n",
    "assert node2parent_test[str(c2)] == str(c1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser states and actions\n",
    "\n",
    "Next, we need a method to construct a dependency tree given a sentence and a list of dependency parse operators. Recall that the operators are:\n",
    "\n",
    "- **SHIFT**: move a word from the input buffer to the stack\n",
    "- **ARC_L**: left arc; add a dependency link from the last element of the stack to the second-to-last element of the stack. remove the second-to-last element of the stack.\n",
    "- **ARC_R**: right arc; add a dependency link from the second-to-last element of the stack to the last element of the stack. remove the last element of the stack.\n",
    "\n",
    "e.g., \n",
    "\n",
    "**Input:**\n",
    "- ['book', 'me', 'the', 'morning', 'flight']\n",
    "- ['SHIFT', 'SHIFT', 'ARC_R', 'SHIFT', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'ARC_R', 'ARC_R']\n",
    "\n",
    "**Output:**\n",
    "\n",
    "The tree\n",
    "\n",
    "```\n",
    "ROOT\n",
    "  --Book\n",
    "    --me\n",
    "    --flight\n",
    "      --the\n",
    "      --morning\n",
    "```\n",
    "\n",
    "To handle this, we'll use an additional data structure that keeps track of the parser state.\n",
    "\n",
    "We'll rely on the [deque](https://docs.python.org/3/library/collections.html#collections.deque) class, which is a double-ended queue for efficient insertion and removal from the head/tail of a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial stack:\n",
      "deque(['a', 'b', 'c'])\n",
      "\n",
      "We can pop the last element with .pop()\n",
      "c\n",
      "leaving\n",
      "deque(['a', 'b'])\n",
      "\n",
      "Like a normal list, we can \"peek\" at any element we like without removing it\n",
      "peek b\n",
      "stack is still\n",
      "deque(['a', 'b'])\n",
      "\n",
      "since it is double-ended, we can also use it like a queue and pop from the left:\n",
      "a\n",
      "this will be useful for storing words in the sentence\n",
      "deque(['b'])\n"
     ]
    }
   ],
   "source": [
    "# example of deque usage.\n",
    "\n",
    "stack = deque() \n",
    "# push elements onto stack\n",
    "stack.append('a')\n",
    "stack.append('b')\n",
    "stack.append('c')\n",
    " \n",
    "print('Initial stack:')\n",
    "print(stack)\n",
    "\n",
    "print('\\nWe can pop the last element with .pop()')\n",
    "print(stack.pop())\n",
    "print('leaving')\n",
    "print(stack)\n",
    "\n",
    "print('\\nLike a normal list, we can \"peek\" at any element we like without removing it')\n",
    "print('peek', stack[-1])\n",
    "print('stack is still')\n",
    "print(stack)\n",
    "# print(stack.pop())\n",
    "# print(stack.pop())\n",
    "\n",
    "print('\\nsince it is double-ended, we can also use it like a queue and pop from the left:')\n",
    "print(stack.popleft())\n",
    "print('this will be useful for storing words in the sentence')\n",
    "print(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input: ['Book-0', 'me-1', 'the-2', 'morning-3', 'flight-4']\n",
       "stack: ['ROOT-0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class State:\n",
    "    \"\"\"\n",
    "    stack....stack of DTree nodes (dequeue)\n",
    "    input....input DTree nodes (dequeue)\n",
    "    root.....reference to the ROOT DTree node\n",
    "    \n",
    "    Note that we initialize the State with a ROOT node, then \n",
    "    push the words in the sentence onto the input deque as DTree objects.\n",
    "    \"\"\"\n",
    "    def __init__(self, sentence):\n",
    "        self.stack = deque()\n",
    "        self.stack.append(DTree('ROOT', 0))\n",
    "        self.input = deque([DTree(w, i) for i, w in enumerate(sentence)])\n",
    "        self.root = self.stack[0]        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'input: %s' % str([str(i) for i in self.input]) + \\\n",
    "               '\\nstack: %s' % str([str(i) for i in self.stack])\n",
    "\n",
    "# we'll use this example sentence a lot.\n",
    "EXAMPLE_SENTENCE = ['Book', 'me', 'the', 'morning', 'flight']\n",
    "state = State(EXAMPLE_SENTENCE)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `State` class, we can now implement `apply_action`. This function will modify the state based on the provided action. We'll assume the action is a string in `['SHIFT', 'ARC_L', 'ARC_R']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53df831634bf2c91363b95b110fc125e",
     "grade": false,
     "grade_id": "apply-action",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state\n",
      "input: ['Book-0', 'me-1', 'the-2', 'morning-3', 'flight-4']\n",
      "stack: ['ROOT-0']\n",
      "\n",
      "Do SHIFT\n",
      "input: ['me-1', 'the-2', 'morning-3', 'flight-4']\n",
      "stack: ['ROOT-0', 'Book-0']\n",
      "\n",
      "Do SHIFT\n",
      "input: ['the-2', 'morning-3', 'flight-4']\n",
      "stack: ['ROOT-0', 'Book-0', 'me-1']\n",
      "\n",
      "Do ARC_L\n",
      "input: ['the-2', 'morning-3', 'flight-4']\n",
      "stack: ['ROOT-0', 'me-1']\n",
      "\n",
      "Do ARC_R\n",
      "input: ['the-2', 'morning-3', 'flight-4']\n",
      "stack: ['ROOT-0']\n",
      "\n",
      "current tree\n",
      "ROOT-0\n",
      "  --me-1\n",
      "    --Book-0\n"
     ]
    }
   ],
   "source": [
    "def apply_action(state, action):\n",
    "    \"\"\"\n",
    "    Modify the state based on the provided action.\n",
    "    Args:\n",
    "      state....a State object\n",
    "      action...a string in ['SHIFT', 'ARC_L', 'ARC_R']\n",
    "    Returns:\n",
    "      nothing. state is modified.\n",
    "    \"\"\"\n",
    "    if action == 'SHIFT':\n",
    "        # YOUR CODE HERE\n",
    "        state.stack.append(state.input[0])\n",
    "        state.input.popleft()\n",
    "        #raise NotImplementedError()\n",
    "    elif action == 'ARC_L':\n",
    "        # YOUR CODE HERE\n",
    "        last=state.stack.pop()\n",
    "        second_last=state.stack.pop()\n",
    "        last.add_child(second_last)\n",
    "        state.stack.append(last)\n",
    "        #raise NotImplementedError()\n",
    "    elif action == 'ARC_R':\n",
    "        # YOUR CODE HERE\n",
    "        last=state.stack.pop()\n",
    "        state.stack[-1].add_child(last)\n",
    "        #raise NotImplementedError()\n",
    "    else:\n",
    "        raise Exception('Unknown action: %s' % action)\n",
    "    \n",
    "\n",
    "state = State(EXAMPLE_SENTENCE)\n",
    "print('Initial state')\n",
    "print(state)\n",
    "\n",
    "print('\\nDo SHIFT')\n",
    "apply_action(state, 'SHIFT')\n",
    "print(state)\n",
    "\n",
    "print('\\nDo SHIFT')\n",
    "apply_action(state, 'SHIFT')\n",
    "print(state)\n",
    "\n",
    "print('\\nDo ARC_L')\n",
    "apply_action(state, 'ARC_L')\n",
    "print(state)\n",
    "\n",
    "print('\\nDo ARC_R')\n",
    "apply_action(state, 'ARC_R')\n",
    "print(state)\n",
    "\n",
    "print('\\ncurrent tree')\n",
    "state.root.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "    \n",
    "```\n",
    "Initial state\n",
    "input: ['Book-0', 'me-1', 'the-2', 'morning-3', 'flight-4']\n",
    "stack: ['ROOT-0']\n",
    "\n",
    "Do SHIFT\n",
    "input: ['me-1', 'the-2', 'morning-3', 'flight-4']\n",
    "stack: ['ROOT-0', 'Book-0']\n",
    "\n",
    "Do SHIFT\n",
    "input: ['the-2', 'morning-3', 'flight-4']\n",
    "stack: ['ROOT-0', 'Book-0', 'me-1']\n",
    "\n",
    "Do ARC_L\n",
    "input: ['the-2', 'morning-3', 'flight-4']\n",
    "stack: ['ROOT-0', 'me-1']\n",
    "\n",
    "Do ARC_R\n",
    "input: ['the-2', 'morning-3', 'flight-4']\n",
    "stack: ['ROOT-0']\n",
    "\n",
    "current tree\n",
    "ROOT-0\n",
    "  --me-1\n",
    "    --Book-0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28edb2e63f0fdaee6de9f028f8839cbb",
     "grade": true,
     "grade_id": "apply-action-test",
     "locked": true,
     "points": 9,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test on our running example.\n",
    "EXAMPLE_ACTIONS = ['SHIFT', 'SHIFT', 'ARC_R', 'SHIFT', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'ARC_R', 'ARC_R']\n",
    "state_test = State(EXAMPLE_SENTENCE)\n",
    "nodes_test = state_test.input.copy()\n",
    "\n",
    "for action in EXAMPLE_ACTIONS:\n",
    "    apply_action(state_test, action)  \n",
    "\n",
    "# test that the final tree looks like this:\n",
    "# ROOT-0\n",
    "#   --Book-1\n",
    "#     --me-2\n",
    "#     --flight-5\n",
    "#       --the-3\n",
    "#       --morning-4\n",
    "\n",
    "assert state_test.root.word == 'ROOT'\n",
    "book = state_test.root.children[0]\n",
    "assert book  == nodes_test[0]\n",
    "\n",
    "me = book.children[0]\n",
    "assert me == nodes_test[1]\n",
    "\n",
    "flight = book.children[1]\n",
    "assert flight == nodes_test[4]\n",
    "\n",
    "# note that \"the\" is the first child of flight and morning is the second, due to the order in which they\n",
    "# are added.\n",
    "the = flight.children[1]\n",
    "assert the == nodes_test[2]\n",
    "\n",
    "morning = flight.children[0]\n",
    "assert morning == nodes_test[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a way to determine when there are no more actions left to perform. I.e., when should we stop calling apply_action? What properties should the State have when we should stop? Implement the method below to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e6741545a1831c3dae82660aefd6989",
     "grade": false,
     "grade_id": "is_finished",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# There's no more action left when stack only has ROOT-0 left, and input is None\n",
    "def is_finished(state):\n",
    "    # YOUR CODE HERE\n",
    "    if len(state.stack)==1 and len(state.input)==0:\n",
    "       return True\n",
    "    else:\n",
    "        return False\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ced632f519a503edaf036e2d48dca7e8",
     "grade": true,
     "grade_id": "is_finished_tests",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "state_test = State(EXAMPLE_SENTENCE)\n",
    "nodes_test = state_test.input.copy()\n",
    "\n",
    "# apply all but the last action and confirm is_finished is False\n",
    "for action in EXAMPLE_ACTIONS[:-1]:\n",
    "    apply_action(state_test, action)  \n",
    "    assert is_finished(state_test) == False\n",
    "\n",
    "# apply final action\n",
    "apply_action(state_test, EXAMPLE_ACTIONS[-1])\n",
    "assert is_finished(state_test) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a helper function to apply all the actions to a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "state after action SHIFT\n",
      "input: ['me-1', 'the-2', 'morning-3', 'flight-4']\n",
      "stack: ['ROOT-0', 'Book-0']\n",
      "\n",
      "state after action SHIFT\n",
      "input: ['the-2', 'morning-3', 'flight-4']\n",
      "stack: ['ROOT-0', 'Book-0', 'me-1']\n",
      "\n",
      "state after action ARC_R\n",
      "input: ['the-2', 'morning-3', 'flight-4']\n",
      "stack: ['ROOT-0', 'Book-0']\n",
      "\n",
      "state after action SHIFT\n",
      "input: ['morning-3', 'flight-4']\n",
      "stack: ['ROOT-0', 'Book-0', 'the-2']\n",
      "\n",
      "state after action SHIFT\n",
      "input: ['flight-4']\n",
      "stack: ['ROOT-0', 'Book-0', 'the-2', 'morning-3']\n",
      "\n",
      "state after action SHIFT\n",
      "input: []\n",
      "stack: ['ROOT-0', 'Book-0', 'the-2', 'morning-3', 'flight-4']\n",
      "\n",
      "state after action ARC_L\n",
      "input: []\n",
      "stack: ['ROOT-0', 'Book-0', 'the-2', 'flight-4']\n",
      "\n",
      "state after action ARC_L\n",
      "input: []\n",
      "stack: ['ROOT-0', 'Book-0', 'flight-4']\n",
      "\n",
      "state after action ARC_R\n",
      "input: []\n",
      "stack: ['ROOT-0', 'Book-0']\n",
      "\n",
      "state after action ARC_R\n",
      "input: []\n",
      "stack: ['ROOT-0']\n",
      "\n",
      "final tree:\n",
      "ROOT-0\n",
      "  --Book-0\n",
      "    --me-1\n",
      "    --flight-4\n",
      "      --morning-3\n",
      "      --the-2\n"
     ]
    }
   ],
   "source": [
    "def apply_actions(sentence, actions, verbose=False):\n",
    "    state = State(sentence)\n",
    "    for action in actions:\n",
    "\n",
    "        if is_finished(state): # stop if needed\n",
    "            break\n",
    "            \n",
    "        apply_action(state, action)\n",
    "        if verbose:\n",
    "            print('\\nstate after action %s' % action)\n",
    "            print(state)\n",
    "    return state\n",
    "\n",
    "# note that in the final answer, \"morning\" and \"the\" are swapped due to the order the dependencies are added.\n",
    "final_state = apply_actions(EXAMPLE_SENTENCE, EXAMPLE_ACTIONS, verbose=True)\n",
    "\n",
    "print('\\nfinal tree:')\n",
    "final_state.root.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we've ignored above is that some actions may be impossible. For example, we can't shift when there are no words left in the input. Likewise, we can't do `ARC_L` or `ARC_R` if there are fewer than two elements on the stack.\n",
    "\n",
    "Implement the method below to check for these cases. The method will return a valid action string if the given action is invalid. \n",
    "\n",
    "If `SHIFT` is illegal, return `ARC_R`.\n",
    "\n",
    "If `ARC_L` or `ARC_R` is illegal, return `SHIFT`.\n",
    "\n",
    "If no moves are legal, raise an exception `raise Exception('no valid action`)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "154e96525a0fb373a6b15960d0a952f6",
     "grade": false,
     "grade_id": "check_action",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['Hi-0', 'there-1']\n",
      "stack: ['ROOT-0']\n",
      "\n",
      "check_action SHIFT\n",
      "input: ['there-1']\n",
      "stack: ['ROOT-0', 'Hi-0']\n",
      "\n",
      "check_action SHIFT\n",
      "input: []\n",
      "stack: ['ROOT-0', 'Hi-0', 'there-1']\n",
      "\n",
      "check_action ARC_R\n",
      "input: []\n",
      "stack: ['ROOT-0', 'Hi-0']\n"
     ]
    }
   ],
   "source": [
    "def check_action(state, action):\n",
    "    # YOUR CODE HERE\n",
    "    if action == 'SHIFT':\n",
    "        if len(state.input)==0:\n",
    "            if len(state.stack)>=2:\n",
    "                return str('ARC_R')\n",
    "            else:\n",
    "                raise Exception('no valid action: %s' % action)\n",
    "        else:\n",
    "            return action\n",
    "    elif action == 'ARC_L' or action == 'ARC_R' :\n",
    "        # YOUR CODE HERE\n",
    "        if len(state.stack) < 2:\n",
    "            if len(state.input)!=0:\n",
    "                return str('SHIFT')\n",
    "            else:\n",
    "                raise Exception('no valid action: %s' % action)\n",
    "        else:\n",
    "            return action\n",
    "    else:\n",
    "        raise Exception('no valid action: %s' % action)    \n",
    "        \n",
    "    #raise NotImplementedError()\n",
    "        \n",
    "state = State(['Hi', 'there'])\n",
    "print(state)\n",
    "\n",
    "print('\\ncheck_action', check_action(state, 'SHIFT'))\n",
    "apply_action(state, 'SHIFT')\n",
    "print(state)\n",
    "\n",
    "print('\\ncheck_action', check_action(state, 'SHIFT'))\n",
    "apply_action(state, 'SHIFT')\n",
    "print(state)\n",
    "\n",
    "print('\\ncheck_action', check_action(state, 'SHIFT'))\n",
    "apply_action(state, 'ARC_R')\n",
    "print(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8706437e08a536c95390ce8eb6022d29",
     "grade": true,
     "grade_id": "check_action_test",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['Hi-0', 'there-1']\n",
      "stack: ['ROOT-0']\n",
      "\n",
      "check_action SHIFT\n",
      "input: ['there-1']\n",
      "stack: ['ROOT-0', 'Hi-0']\n",
      "\n",
      "check_action SHIFT\n",
      "input: []\n",
      "stack: ['ROOT-0', 'Hi-0', 'there-1']\n",
      "\n",
      "check_action ARC_R\n",
      "input: []\n",
      "stack: ['ROOT-0', 'Hi-0']\n"
     ]
    }
   ],
   "source": [
    "state = State(['Hi', 'there'])\n",
    "print(state)\n",
    "\n",
    "print('\\ncheck_action', check_action(state, 'SHIFT'))\n",
    "assert check_action(state, 'SHIFT') == 'SHIFT'\n",
    "apply_action(state, 'SHIFT')\n",
    "print(state)\n",
    "\n",
    "\n",
    "print('\\ncheck_action', check_action(state, 'SHIFT'))\n",
    "assert check_action(state, 'SHIFT') == 'SHIFT'\n",
    "apply_action(state, 'SHIFT')\n",
    "print(state)\n",
    "\n",
    "\n",
    "print('\\ncheck_action', check_action(state, 'SHIFT'))\n",
    "assert check_action(state, 'SHIFT') == 'ARC_R'\n",
    "apply_action(state, 'ARC_R')\n",
    "print(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an action classifier \n",
    "\n",
    "OK!  We now have some infrastructure in place to apply a sequence of actions to create a dependency tree.\n",
    "\n",
    "Next, we'll need to train a classifier to predict the best action at each time step. To do this, we'll use some labeled data. Each example contains a sentence and the sequence of true actions to get the correct dependency tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 10001 sentences\n",
      "here is the fifth example\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': ['The',\n",
       "  'third',\n",
       "  'was',\n",
       "  'being',\n",
       "  'run',\n",
       "  'by',\n",
       "  'the',\n",
       "  'head',\n",
       "  'of',\n",
       "  'an',\n",
       "  'investment',\n",
       "  'firm',\n",
       "  '.'],\n",
       " 'actions': ['SHIFT',\n",
       "  'SHIFT',\n",
       "  'ARC_L',\n",
       "  'SHIFT',\n",
       "  'SHIFT',\n",
       "  'SHIFT',\n",
       "  'ARC_L',\n",
       "  'ARC_L',\n",
       "  'ARC_L',\n",
       "  'SHIFT',\n",
       "  'SHIFT',\n",
       "  'SHIFT',\n",
       "  'ARC_L',\n",
       "  'ARC_L',\n",
       "  'SHIFT',\n",
       "  'SHIFT',\n",
       "  'SHIFT',\n",
       "  'SHIFT',\n",
       "  'ARC_L',\n",
       "  'ARC_L',\n",
       "  'ARC_L',\n",
       "  'ARC_R',\n",
       "  'ARC_R',\n",
       "  'SHIFT',\n",
       "  'ARC_R',\n",
       "  'ARC_R']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(fname):\n",
    "    data = []    \n",
    "    for line in open(fname):\n",
    "        sentence, actions = line.split(\"\\t\")\n",
    "        sentence = sentence.strip().split()\n",
    "        actions = actions.strip().split()\n",
    "        data.append({'sentence': sentence, 'actions': actions})\n",
    "    return data\n",
    "\n",
    "train_data = read_data(os.path.join('data', 'train.txt'))\n",
    "\n",
    "print('read %d sentences' % len(train_data))\n",
    "\n",
    "print('here is the fifth example')\n",
    "train_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT-0\n",
      "  --run-4\n",
      "    --being-3\n",
      "    --was-2\n",
      "    --third-1\n",
      "      --The-0\n",
      "    --head-7\n",
      "      --the-6\n",
      "      --by-5\n",
      "      --firm-11\n",
      "        --investment-10\n",
      "        --an-9\n",
      "        --of-8\n",
      "    --.-12\n"
     ]
    }
   ],
   "source": [
    "# and here we can use apply_actions apply the true actions to get the final dependency tree\n",
    "final_state = apply_actions(train_data[4]['sentence'], train_data[4]['actions'], verbose=False)\n",
    "final_state.root.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data\n",
    "\n",
    "To generate training examples, we will loop over each example, then apply each true action.\n",
    "\n",
    "At each step, we will yield a `(state, action)` tuple indicating what the correct action is for that state.\n",
    "\n",
    "The `state` will be used to compute features, and the `action` will become our class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:\n",
      "input: ['Al-0', '--1', 'Zaman-2', ':-3', 'American-4', 'forces-5', 'killed-6', 'Shaikh-7', 'Abdullah-8', 'al-9', '--10', 'Ani-11', ',-12', 'the-13', 'preacher-14', 'at-15', 'the-16', 'mosque-17', 'in-18', 'the-19', 'town-20', 'of-21', 'Qaim-22', ',-23', 'near-24', 'the-25', 'Syrian-26', 'border-27', '.-28']\n",
      "stack: ['ROOT-0']\n",
      "action:\n",
      "SHIFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_training_samples(data):\n",
    "    for d in tqdm(data):\n",
    "        sentence = d['sentence'].copy()\n",
    "        actions = d['actions'].copy()\n",
    "        state = State(sentence)\n",
    "        for action in actions:\n",
    "            # we use yield here so this becomes an iterator\n",
    "            # this can save memory, as opposed to constructing a \n",
    "            # giant list before compute the feature vectors.\n",
    "            yield(state, action)\n",
    "            apply_action(state, action)\n",
    "\n",
    "# here is the first training sample generated\n",
    "state, action = next(generate_training_samples(train_data))\n",
    "print('state:')\n",
    "print(state)\n",
    "print('action:')\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute features\n",
    "\n",
    "While we iterate over the training samples, we will compute a list of features over each state.\n",
    "\n",
    "A feature will be a tuple of (`feature_name, feature_value`). For most features, the `feature_value` will be 1.0.\n",
    "\n",
    "For example, here is a feature representing the word at the top of the stack (position -1)\n",
    "\n",
    "We use the special token `EMPTY` to indicate that there is no word present at the given position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('STACK@-1=ROOT', 1.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stack_feature(state):\n",
    "    if len(state.stack) == 0:\n",
    "        return ('STACK@-1=EMPTY', 1.0)\n",
    "    else:\n",
    "        return ('STACK@-1=%s' % state.stack[-1].word, 1.0)\n",
    "    \n",
    "stack_feature(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, here is a feature representing the word at the front of the input (position 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('INPUT@0=Al', 1.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def input_feature(state):\n",
    "    if len(state.input) == 0:\n",
    "        return ('INPUT@0=EMPTY', 1.0)\n",
    "    else:\n",
    "        return ('INPUT@0=%s' % state.input[0].word, 1.0)\n",
    "    \n",
    "input_feature(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a function that iterates over all the training samples generated by `generate_training_samples`, computes all feature functions, and stores the result in a list. Along the way, we'll also keep track of the true actions corresponding to each feature list.\n",
    "\n",
    "Below, we test this for the first two examples in the train_data. As you can see, even with just two sentences, this generates a lot of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 3052.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('SHIFT', [('STACK@-1=ROOT', 1.0), ('INPUT@0=Al', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Al', 1.0), ('INPUT@0=-', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=-', 1.0), ('INPUT@0=Zaman', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Al', 1.0), ('INPUT@0=Zaman', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=Zaman', 1.0), ('INPUT@0=:', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Al', 1.0), ('INPUT@0=:', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=:', 1.0), ('INPUT@0=American', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Al', 1.0), ('INPUT@0=American', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=American', 1.0), ('INPUT@0=forces', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=forces', 1.0), ('INPUT@0=killed', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=forces', 1.0), ('INPUT@0=killed', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=killed', 1.0), ('INPUT@0=Shaikh', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=killed', 1.0), ('INPUT@0=Shaikh', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Shaikh', 1.0), ('INPUT@0=Abdullah', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=Abdullah', 1.0), ('INPUT@0=al', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Shaikh', 1.0), ('INPUT@0=al', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=al', 1.0), ('INPUT@0=-', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Shaikh', 1.0), ('INPUT@0=-', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=-', 1.0), ('INPUT@0=Ani', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Shaikh', 1.0), ('INPUT@0=Ani', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=Ani', 1.0), ('INPUT@0=,', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Shaikh', 1.0), ('INPUT@0=,', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=,', 1.0), ('INPUT@0=the', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Shaikh', 1.0), ('INPUT@0=the', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=the', 1.0), ('INPUT@0=preacher', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=preacher', 1.0), ('INPUT@0=at', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=preacher', 1.0), ('INPUT@0=at', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=Shaikh', 1.0), ('INPUT@0=at', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=killed', 1.0), ('INPUT@0=at', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=at', 1.0), ('INPUT@0=the', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=the', 1.0), ('INPUT@0=mosque', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=mosque', 1.0), ('INPUT@0=in', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=mosque', 1.0), ('INPUT@0=in', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=mosque', 1.0), ('INPUT@0=in', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=in', 1.0), ('INPUT@0=the', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=the', 1.0), ('INPUT@0=town', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=town', 1.0), ('INPUT@0=of', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=town', 1.0), ('INPUT@0=of', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=town', 1.0), ('INPUT@0=of', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=of', 1.0), ('INPUT@0=Qaim', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=Qaim', 1.0), ('INPUT@0=,', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=Qaim', 1.0), ('INPUT@0=,', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=town', 1.0), ('INPUT@0=,', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=,', 1.0), ('INPUT@0=near', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=town', 1.0), ('INPUT@0=near', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=near', 1.0), ('INPUT@0=the', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=the', 1.0), ('INPUT@0=Syrian', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Syrian', 1.0), ('INPUT@0=border', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=border', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=border', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=border', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=border', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=town', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=mosque', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=killed', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=Al', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=.', 1.0), ('INPUT@0=EMPTY', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=Al', 1.0), ('INPUT@0=EMPTY', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=ROOT', 1.0), ('INPUT@0=[', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=[', 1.0), ('INPUT@0=This', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=This', 1.0), ('INPUT@0=killing', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=killing', 1.0), ('INPUT@0=of', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=killing', 1.0), ('INPUT@0=of', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=of', 1.0), ('INPUT@0=a', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=a', 1.0), ('INPUT@0=respected', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=respected', 1.0), ('INPUT@0=cleric', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=cleric', 1.0), ('INPUT@0=will', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=cleric', 1.0), ('INPUT@0=will', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=cleric', 1.0), ('INPUT@0=will', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=cleric', 1.0), ('INPUT@0=will', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=killing', 1.0), ('INPUT@0=will', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=will', 1.0), ('INPUT@0=be', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=be', 1.0), ('INPUT@0=causing', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=causing', 1.0), ('INPUT@0=us', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=causing', 1.0), ('INPUT@0=us', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=causing', 1.0), ('INPUT@0=us', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=causing', 1.0), ('INPUT@0=us', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=causing', 1.0), ('INPUT@0=us', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=us', 1.0), ('INPUT@0=trouble', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=causing', 1.0), ('INPUT@0=trouble', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=trouble', 1.0), ('INPUT@0=for', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=causing', 1.0), ('INPUT@0=for', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=for', 1.0), ('INPUT@0=years', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=years', 1.0), ('INPUT@0=to', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=years', 1.0), ('INPUT@0=to', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=to', 1.0), ('INPUT@0=come', 1.0)]),\n",
       " ('ARC_L', [('STACK@-1=come', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=come', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=years', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=causing', 1.0), ('INPUT@0=.', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=.', 1.0), ('INPUT@0=]', 1.0)]),\n",
       " ('SHIFT', [('STACK@-1=causing', 1.0), ('INPUT@0=]', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=]', 1.0), ('INPUT@0=EMPTY', 1.0)]),\n",
       " ('ARC_R', [('STACK@-1=causing', 1.0), ('INPUT@0=EMPTY', 1.0)])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_features(sample_iter, feature_fns):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for state, action in sample_iter:\n",
    "        features.append([feature_fn(state) for feature_fn in feature_fns])\n",
    "        labels.append(action)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "feature_fns = [stack_feature, input_feature]\n",
    "features, labels = compute_features(generate_training_samples(train_data[:2]),\n",
    "                                    feature_fns)\n",
    "display(list(zip(labels, features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a few more feature functions. Complete the methods below, again using `EMPTY` to indicate that there is no word present at the given position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2806160cf359983542f89084a102d3a",
     "grade": false,
     "grade_id": "features",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def stack_feature_2(state):\n",
    "    \"\"\"\n",
    "    Return a feature for the second-to-last element of the stack.\n",
    "    e.g., (STACK@-2=years, 1.0)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if len(state.stack) < 2:\n",
    "        return ('STACK@-2=EMPTY', 1.0)\n",
    "    else:\n",
    "        return ('STACK@-2=%s' % state.stack[-2].word, 1.0)\n",
    "    \n",
    "    #raise NotImplementedError()\n",
    "\n",
    "def input_feature_2(state):\n",
    "    \"\"\"\n",
    "    Return a feature for the second element of the input.\n",
    "    e.g., (INPUT@1=years, 1.0)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if len(state.input) < 2:\n",
    "        return ('INPUT@1=EMPTY', 1.0)\n",
    "    else:\n",
    "        return ('INPUT@1=%s' % state.input[1].word, 1.0)\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "def stack_pair_feature(state):\n",
    "    \"\"\"\n",
    "    A combination of the last and second-to-last elements of the stack.\n",
    "    e.g., STACK@-1=being__STACK@-2=was\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    f1=stack_feature(state)[0]\n",
    "    f2=stack_feature_2(state)[0]\n",
    "    return ('%s__%s' % (f1, f2), 1.0)\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65cb7dc934cbd39a2c8edfd457b21dd3",
     "grade": true,
     "grade_id": "features-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['Book-0', 'me-1', 'the-2', 'morning-3', 'flight-4']\n",
      "stack: ['ROOT-0']\n",
      "('STACK@-1=ROOT', 1.0)\n",
      "('STACK@-2=EMPTY', 1.0)\n",
      "('INPUT@0=Book', 1.0)\n",
      "('INPUT@1=me', 1.0)\n",
      "('STACK@-1=ROOT__STACK@-2=EMPTY', 1.0)\n",
      "\n",
      "state after SHIFT\n",
      "input: ['me-1', 'the-2', 'morning-3', 'flight-4']\n",
      "stack: ['ROOT-0', 'Book-0']\n",
      "('STACK@-1=Book', 1.0)\n",
      "('STACK@-2=ROOT', 1.0)\n",
      "('INPUT@0=me', 1.0)\n",
      "('INPUT@1=the', 1.0)\n",
      "('STACK@-1=Book__STACK@-2=ROOT', 1.0)\n"
     ]
    }
   ],
   "source": [
    "state = State(EXAMPLE_SENTENCE)\n",
    "print(state)\n",
    "print(stack_feature(state))\n",
    "assert stack_feature(state) == ('STACK@-1=ROOT', 1.0)\n",
    "print(stack_feature_2(state))\n",
    "assert stack_feature_2(state) == ('STACK@-2=EMPTY', 1.0)\n",
    "print(input_feature(state))\n",
    "assert input_feature(state) == ('INPUT@0=Book', 1.0)\n",
    "print(input_feature_2(state))\n",
    "assert input_feature_2(state) == ('INPUT@1=me', 1.0)\n",
    "print(stack_pair_feature(state))\n",
    "assert stack_pair_feature(state) == ('STACK@-1=ROOT__STACK@-2=EMPTY', 1.0)\n",
    "apply_action(state, 'SHIFT')\n",
    "\n",
    "print()\n",
    "print('state after SHIFT')\n",
    "print(state)\n",
    "print(stack_feature(state))\n",
    "print(stack_feature_2(state))\n",
    "print(input_feature(state))\n",
    "print(input_feature_2(state))\n",
    "print(stack_pair_feature(state))\n",
    "assert stack_pair_feature(state) == ('STACK@-1=Book__STACK@-2=ROOT', 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2129.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('SHIFT',\n",
       "  [('STACK@-1=ROOT', 1.0),\n",
       "   ('INPUT@0=Al', 1.0),\n",
       "   ('STACK@-2=EMPTY', 1.0),\n",
       "   ('INPUT@1=-', 1.0),\n",
       "   ('STACK@-1=ROOT__STACK@-2=EMPTY', 1.0)]),\n",
       " ('SHIFT',\n",
       "  [('STACK@-1=Al', 1.0),\n",
       "   ('INPUT@0=-', 1.0),\n",
       "   ('STACK@-2=ROOT', 1.0),\n",
       "   ('INPUT@1=Zaman', 1.0),\n",
       "   ('STACK@-1=Al__STACK@-2=ROOT', 1.0)]),\n",
       " ('ARC_R',\n",
       "  [('STACK@-1=-', 1.0),\n",
       "   ('INPUT@0=Zaman', 1.0),\n",
       "   ('STACK@-2=Al', 1.0),\n",
       "   ('INPUT@1=:', 1.0),\n",
       "   ('STACK@-1=-__STACK@-2=Al', 1.0)]),\n",
       " ('SHIFT',\n",
       "  [('STACK@-1=Al', 1.0),\n",
       "   ('INPUT@0=Zaman', 1.0),\n",
       "   ('STACK@-2=ROOT', 1.0),\n",
       "   ('INPUT@1=:', 1.0),\n",
       "   ('STACK@-1=Al__STACK@-2=ROOT', 1.0)]),\n",
       " ('ARC_R',\n",
       "  [('STACK@-1=Zaman', 1.0),\n",
       "   ('INPUT@0=:', 1.0),\n",
       "   ('STACK@-2=Al', 1.0),\n",
       "   ('INPUT@1=American', 1.0),\n",
       "   ('STACK@-1=Zaman__STACK@-2=Al', 1.0)])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here are the new features for the first 5 instances.\n",
    "feature_fns = [stack_feature, input_feature, stack_feature_2, input_feature_2, stack_pair_feature]\n",
    "features, labels = compute_features(generate_training_samples(train_data[:2]),\n",
    "                                    feature_fns)\n",
    "display(list(zip(labels, features))[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "[('SHIFT',\n",
    "  [('STACK@-1=ROOT', 1.0),\n",
    "   ('INPUT@0=Al', 1.0),\n",
    "   ('STACK@-2=EMPTY', 1.0),\n",
    "   ('INPUT@1=-', 1.0),\n",
    "   ('STACK@-1=ROOT__STACK@-2=EMPTY', 1)]),\n",
    " ('SHIFT',\n",
    "  [('STACK@-1=Al', 1.0),\n",
    "   ('INPUT@0=-', 1.0),\n",
    "   ('STACK@-2=ROOT', 1.0),\n",
    "   ('INPUT@1=Zaman', 1.0),\n",
    "   ('STACK@-1=Al__STACK@-2=ROOT', 1)]),\n",
    " ('ARC_R',\n",
    "  [('STACK@-1=-', 1.0),\n",
    "   ('INPUT@0=Zaman', 1.0),\n",
    "   ('STACK@-2=Al', 1.0),\n",
    "   ('INPUT@1=:', 1.0),\n",
    "   ('STACK@-1=-__STACK@-2=Al', 1)]),\n",
    " ('SHIFT',\n",
    "  [('STACK@-1=Al', 1.0),\n",
    "   ('INPUT@0=Zaman', 1.0),\n",
    "   ('STACK@-2=ROOT', 1.0),\n",
    "   ('INPUT@1=:', 1.0),\n",
    "   ('STACK@-1=Al__STACK@-2=ROOT', 1)]),\n",
    " ('ARC_R',\n",
    "  [('STACK@-1=Zaman', 1.0),\n",
    "   ('INPUT@0=:', 1.0),\n",
    "   ('STACK@-2=Al', 1.0),\n",
    "   ('INPUT@1=American', 1.0),\n",
    "   ('STACK@-1=Zaman__STACK@-2=Al', 1)])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit classifier\n",
    "\n",
    "To train the classifier, we'll first compute all the features, then use [DictVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) to create a sparse feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [00:02<00:00, 4312.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(341552, 224483)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = compute_features(\n",
    "    generate_training_samples(train_data),\n",
    "    [stack_feature, stack_feature_2, stack_pair_feature, input_feature, input_feature_2])\n",
    "vec = DictVectorizer()\n",
    "X_train = vec.fit_transform(dict(f) for f in features)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=100, class_weight='balanced')\n",
    "clf.fit(X_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARC_L\n",
      "INPUT@0=EMPTY 14.524752345198065\n",
      "STACK@-2=very 5.749726444062386\n",
      "STACK@-2=There 5.490462562135689\n",
      "STACK@-2=, 5.339117916469915\n",
      "STACK@-2=for 5.218886682806136\n",
      "STACK@-2=with 4.852941198002602\n",
      "STACK@-2=I 4.783636518915244\n",
      "STACK@-2=a 4.648052015707228\n",
      "STACK@-2=to 4.486867340963744\n",
      "STACK@-2=It 4.434911564979165\n",
      "STACK@-2=/ 4.354319311404713\n",
      "STACK@-2=from 4.281199744114217\n",
      "STACK@-2=the 4.2444744722911585\n",
      "STACK@-2=or 4.040197645179698\n",
      "STACK@-2=by 3.9891510841024367\n",
      "ARC_R\n",
      "INPUT@0=EMPTY 15.853561675978884\n",
      "STACK@-1=that__STACK@-2=so 6.200443574856145\n",
      "STACK@-1=of__STACK@-2=because 6.119218690841352\n",
      "STACK@-1=, 5.37541983010691\n",
      "STACK@-1=course__STACK@-2=of 5.064098349705973\n",
      "STACK@-1=? 5.033751414205209\n",
      "STACK@-1=: 4.932816102510562\n",
      "STACK@-1=as__STACK@-2=as 4.707892838417774\n",
      "STACK@-1=! 4.2873600801444365\n",
      "STACK@-1=s 4.037135091787385\n",
      "STACK@-1=as__STACK@-2=such 4.0125661209749275\n",
      "STACK@-1=well__STACK@-2=as 3.9895687946077243\n",
      "STACK@-1=\" 3.978935333197669\n",
      "STACK@-1=; 3.9152903410641375\n",
      "STACK@-1=- 3.8692112481410947\n",
      "SHIFT\n",
      "STACK@-2=ROOT 16.62213837825694\n",
      "STACK@-1=and 8.916678844680968\n",
      "STACK@-1=a 7.1655604567039495\n",
      "STACK@-1=ROOT__STACK@-2=EMPTY 7.138281271557126\n",
      "STACK@-2=EMPTY 7.138281271557126\n",
      "STACK@-1=ROOT 7.138281271557126\n",
      "STACK@-1=the 7.033782845384073\n",
      "STACK@-1=your 5.753074151189698\n",
      "STACK@-1=my 5.460128944204372\n",
      "STACK@-1=an 5.328292545730068\n",
      "INPUT@0=PM 5.1970394976871415\n",
      "STACK@-1=if 5.0551081541748495\n",
      "STACK@-1=, 4.797947416239994\n",
      "STACK@-1=just 4.747560375500521\n",
      "STACK@-1=we 4.613858582167075\n"
     ]
    }
   ],
   "source": [
    "# print top coefficients for each class.\n",
    "feats = npa(vec.get_feature_names())\n",
    "for ci, cname in enumerate(clf.classes_):\n",
    "    print(cname)\n",
    "    theta = clf.coef_[ci]\n",
    "    for i in np.argsort(theta)[::-1][:15]:\n",
    "        print(feats[i], theta[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHORT ANSWER**: \n",
    "\n",
    "**Look at the top coefficients for each class. Do any of the top features make sense to you? Pick two features and explain why you think they are highly weighted, along with examples of each. Enter your answer below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e91b070bcf2552b9cdc3315252152280",
     "grade": true,
     "grade_id": "cell-f936ecdf28ec0e71",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "For the 'SHIFT' class, STACK@-2=ROOT, STACK@-1=ROOT__STACK@-2=EMPTY, STACK@-2=EMPTY, STACK@-1=ROOT appear at the top features, since they indicate the start of one sentence, they need 'SHIFT' operation to push another work into the stack. For 'ARC_R' class, there are lots of punctuations, which represent the end of one sentence, they need a 'ARC_R' action to produce a head-dependent relation and all of the dependent words at the top of the stack have been assigned, which usually happend at the end of a list of actions. Also, I noticed INPUT@0=EMPTY is the top 1 feature in either 'ARC_L' or 'ARC_R', because it represents the input is empty and no ‘SHIFT’ action allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the classifier to parse\n",
    "\n",
    "With the classifier `clf`, we can now predict a good action to take for any possible state.\n",
    "\n",
    "To do so, we'll create a new class called `ClassifierParser`, which will apply the lcassifier to parse a given sentence.\n",
    "\n",
    "For any given state, we'll need to (i) compute the feature functions, (ii) transform the feature functions into a sparse vector, (iii) call `clf.predict` to get the most probable action for the given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1d4b1da3813d2fba127ace7e1adc9e9",
     "grade": false,
     "grade_id": "parser",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predicted actions\n",
      "['SHIFT', 'SHIFT', 'ARC_R', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'SHIFT', 'ARC_R', 'ARC_R']\n",
      "\n",
      "true actions\n",
      "['SHIFT', 'SHIFT', 'ARC_R', 'SHIFT', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'ARC_R', 'ARC_R']\n",
      "\n",
      "predicted tree\n",
      "ROOT-0\n",
      "  --morning-3\n",
      "    --the-2\n",
      "    --Book-0\n",
      "      --me-1\n",
      "    --flight-4\n",
      "\n",
      "true tree\n",
      "ROOT-0\n",
      "  --Book-0\n",
      "    --me-1\n",
      "    --flight-4\n",
      "      --morning-3\n",
      "      --the-2\n"
     ]
    }
   ],
   "source": [
    "class ClassifierParser:\n",
    "    def __init__(self, clf, vec, feature_fns):\n",
    "        self.clf = clf\n",
    "        self.vec = vec\n",
    "        self.feature_fns = feature_fns\n",
    "        \n",
    "    def compute_features(self, state):\n",
    "        \"\"\"\n",
    "        Apply all of the `feature_fns` to this state, then\n",
    "        call `vec.transform` to convert them into a csr_matrix with a single row.\n",
    "        Return this matrix.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        features.append([feature_fn(state) for feature_fn in feature_fns])\n",
    "        #vec = DictVectorizer()\n",
    "        X_test = self.vec.transform(dict(f) for f in features) \n",
    "        return X_test\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def predict_action(self, state):  \n",
    "        \"\"\"\n",
    "        First, call `compute_features` to get the feature matrix for this single instance.\n",
    "        Then, return the predicted action for this sequence.\n",
    "        The returned value should be a single string in ['SHIFT', 'ARC_L', 'ARC_R']\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        x_test=self.compute_features(state)\n",
    "        pred=clf.predict(x_test)\n",
    "        return pred\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "    def parse(self, sentence, verbose=False):\n",
    "        \"\"\"\n",
    "        Putting it all together, here we will:\n",
    "        - Create an initial State for this sentence\n",
    "        - while the state is not finished:\n",
    "          + call `predict_action` to get the next action to take\n",
    "          + call `check_action` to ensure the action is valid\n",
    "          + apply the action\n",
    "        \n",
    "        Return: a tuple:\n",
    "          state.....the final State at the end of parsing\n",
    "          actions...the list of actions taken\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        state_test = State(sentence)\n",
    "        actions=[]\n",
    "        #i=0\n",
    "        while is_finished(state_test) == False:\n",
    "#               i+=1\n",
    "#               print(i)\n",
    "            pred=self.predict_action(state_test)\n",
    "            che_act=check_action(state_test, pred)\n",
    "            #print(check_action(state_test, pred))\n",
    "            apply_action(state_test, che_act)\n",
    "            actions.append(che_act[0])\n",
    "#               print(state_test)\n",
    "#               print(actions)\n",
    "            \n",
    "        return state_test, actions\n",
    "        #raise NotImplementedError()\n",
    "    \n",
    "# we don't get this example exactly right\n",
    "parser = ClassifierParser(clf, vec, feature_fns)\n",
    "state, actions = parser.parse(EXAMPLE_SENTENCE)\n",
    "print('\\npredicted actions')\n",
    "print(actions)\n",
    "print('\\ntrue actions')\n",
    "print(EXAMPLE_ACTIONS)\n",
    "\n",
    "print('\\npredicted tree')\n",
    "state.root.print()\n",
    "print('\\ntrue tree')\n",
    "apply_actions(EXAMPLE_SENTENCE, EXAMPLE_ACTIONS, verbose=False).root.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output (some variations expected given slightly different classifier parameters)\n",
    "\n",
    "```\n",
    "predicted actions\n",
    "['SHIFT', 'SHIFT', 'ARC_R', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'SHIFT', 'ARC_R', 'ARC_R']\n",
    "\n",
    "true actions\n",
    "['SHIFT', 'SHIFT', 'ARC_R', 'SHIFT', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'ARC_R', 'ARC_R']\n",
    "\n",
    "predicted tree\n",
    "ROOT-0\n",
    "  --morning-3\n",
    "    --the-2\n",
    "    --Book-0\n",
    "      --me-1\n",
    "    --flight-4\n",
    "\n",
    "true tree\n",
    "ROOT-0\n",
    "  --Book-0\n",
    "    --me-1\n",
    "    --flight-4\n",
    "      --morning-3\n",
    "      --the-2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predicted actions\n",
      "['SHIFT', 'SHIFT', 'ARC_L', 'SHIFT', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'ARC_L', 'SHIFT', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'ARC_L', 'ARC_R', 'ARC_R', 'SHIFT', 'ARC_R', 'ARC_R']\n",
      "\n",
      "true actions\n",
      "['SHIFT', 'SHIFT', 'ARC_L', 'SHIFT', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'ARC_L', 'SHIFT', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'ARC_L', 'ARC_L', 'ARC_L', 'ARC_R', 'ARC_R', 'SHIFT', 'ARC_R', 'ARC_R']\n",
      "\n",
      "predicted tree\n",
      "ROOT-0\n",
      "  --run-4\n",
      "    --being-3\n",
      "    --was-2\n",
      "    --third-1\n",
      "      --The-0\n",
      "    --head-7\n",
      "      --the-6\n",
      "      --by-5\n",
      "      --firm-11\n",
      "        --investment-10\n",
      "        --an-9\n",
      "        --of-8\n",
      "    --.-12\n",
      "\n",
      "true tree\n",
      "ROOT-0\n",
      "  --run-4\n",
      "    --being-3\n",
      "    --was-2\n",
      "    --third-1\n",
      "      --The-0\n",
      "    --head-7\n",
      "      --the-6\n",
      "      --by-5\n",
      "      --firm-11\n",
      "        --investment-10\n",
      "        --an-9\n",
      "        --of-8\n",
      "    --.-12\n"
     ]
    }
   ],
   "source": [
    "state, actions = parser.parse(train_data[4]['sentence'])\n",
    "print('\\npredicted actions')\n",
    "print(actions)\n",
    "print('\\ntrue actions')\n",
    "print(train_data[4]['actions'])\n",
    "\n",
    "print('\\npredicted tree')\n",
    "state.root.print()\n",
    "print('\\ntrue tree')\n",
    "apply_actions(train_data[4]['sentence'], train_data[4]['actions'], verbose=False).root.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To measure the accuracy of the parsers, we will first read in the test data, then generate parses for each sentence.\n",
    "Using our `get_parents` method from above, we'll then compare the parent links in the predicted and true trees.\n",
    "\n",
    "Our measure will be the fraction of predicted parent links that are present in the true tree.\n",
    "\n",
    "E.g., if the predicted tree contains edges a->b, b->c, c->d, but the true tree has edges a->b, b->c, b->d, then the score will be 2/3.\n",
    "\n",
    "We'll compute the accuracy by iterating through every tree and summing together the numerator and denominator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 501 test sentences\n"
     ]
    }
   ],
   "source": [
    "test_data = read_data(os.path.join('data', 'test.txt'))\n",
    "print('read %d test sentences' % len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72df7e5fefaebbae6821b827d93c153b",
     "grade": false,
     "grade_id": "evaluate",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5813454499558248"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(parser, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate the parser on this test data using the instructions above.\n",
    "    Return:\n",
    "      a float indicating the edge accuracy.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    denominator_sum=0\n",
    "    numerator_sum=0\n",
    "    for data in test_data:\n",
    "        #prediction\n",
    "        state, actions = parser.parse(data['sentence'])\n",
    "        node2parent_predict = {}\n",
    "        parents_predict =get_parents(state.root, node2parent_predict)\n",
    "        \n",
    "        #ground truth\n",
    "        temp=apply_actions(data['sentence'], data['actions'], verbose=False).root\n",
    "        node2parent_test = {}\n",
    "        parents_test =get_parents(temp, node2parent_test)\n",
    "        \n",
    "        #calculate how many equal pairs\n",
    "        common_pairs=0\n",
    "        for key in parents_test:\n",
    "            if (key in parents_predict and parents_test[key] == parents_predict[key]):\n",
    "                common_pairs+= 1\n",
    "        denominator_sum+=max(len(parents_predict),len(parents_test))\n",
    "        numerator_sum+=common_pairs\n",
    "    ratio=numerator_sum/denominator_sum\n",
    "    return ratio\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "evaluate(parser, test_data)\n",
    "# I get around 0.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7716c3367f023d93c620d49d0384ff14",
     "grade": true,
     "grade_id": "evaluate-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create an oracle classifier for testing the evaluation method on the first test example.\n",
    "# it should have an accuracy of 1.0, since it just applies the true actions.\n",
    "class OracleParser:\n",
    "    def __init__(self, true_actions):\n",
    "        self.true_actions = true_actions\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        return apply_actions(sentence, self.true_actions), self.true_actions       \n",
    "        \n",
    "assert evaluate(OracleParser(test_data[0]['actions']), test_data[:1]) == 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Feature engineering\n",
    "\n",
    "We now have a working dependency parser. The accuracy is actually not too bad, given how few features we're using.\n",
    "\n",
    "For 5 extra points, spend some time on feature engineering and parameter tuning to see if you can improve accuracy. You may want to iterate on a somewhat smaller version of the dataset to save time before applying to the larger dataset. Explain what you did, how much of a difference it made, and why you think it helped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1aa94f4f127669706a303db6cfd87a69",
     "grade": true,
     "grade_id": "cell-3d31d788f2e44ed4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Summary:\n",
    "- add input_pair_feature(a combination of the first and second elements of the input), accuracy=0.5789473684210527\n",
    "- add stack_feature_i1(a feature for the first element of the stack), accuracy=0.5427237157642307\n",
    "- remove stack_pair_feature, accuracy=0.5723841979048341\n",
    "- remove input_feature_2, accuracy=0.5954815095292187 --> best accuracy I can get\n",
    "- remove stack_feature_2, accuracy=0.06550549034456646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pair_feature(state):\n",
    "    \"\"\"\n",
    "    A combination of the first and second elements of the input.\n",
    "    e.g., INPUT@0=being__INPUT@1=was\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    f1=input_feature(state)[0]\n",
    "    f2=input_feature_2(state)[0]\n",
    "    return ('%s__%s' % (f1, f2), 1.0)\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [00:04<00:00, 2497.65it/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = compute_features(\n",
    "    generate_training_samples(train_data),\n",
    "    [stack_feature, stack_feature_2, stack_pair_feature, input_feature, input_feature_2,input_pair_feature])\n",
    "vec = DictVectorizer()\n",
    "X_train = vec.fit_transform(dict(f) for f in features)\n",
    "X_train.shape\n",
    "clf = LogisticRegression(max_iter=100, class_weight='balanced')\n",
    "clf.fit(X_train, labels)\n",
    "parser = ClassifierParser(clf, vec, feature_fns)\n",
    "evaluate(parser, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_feature_i1(state):\n",
    "    \"\"\"\n",
    "    Return a feature for the first element of the stack.\n",
    "    e.g., (STACK@0=years, 1.0)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if len(state.stack) != 0:\n",
    "        return ('STACK@0=EMPTY', 1.0)\n",
    "    else:\n",
    "        return ('STACK@0=%s' % state.stack[0].word, 1.0)\n",
    "    \n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [00:02<00:00, 3828.49it/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5427237157642307"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = compute_features(\n",
    "    generate_training_samples(train_data),\n",
    "    [stack_feature, stack_feature_2, stack_pair_feature, input_feature, input_feature_2,stack_feature_i1])\n",
    "vec = DictVectorizer()\n",
    "X_train = vec.fit_transform(dict(f) for f in features)\n",
    "X_train.shape\n",
    "clf = LogisticRegression(max_iter=100, class_weight='balanced')\n",
    "clf.fit(X_train, labels)\n",
    "parser = ClassifierParser(clf, vec, feature_fns)\n",
    "evaluate(parser, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [00:01<00:00, 5349.67it/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5723841979048341"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = compute_features(\n",
    "    generate_training_samples(train_data),\n",
    "    [stack_feature, stack_feature_2, input_feature, input_feature_2])\n",
    "vec = DictVectorizer()\n",
    "X_train = vec.fit_transform(dict(f) for f in features)\n",
    "X_train.shape\n",
    "clf = LogisticRegression(max_iter=100, class_weight='balanced')\n",
    "clf.fit(X_train, labels)\n",
    "parser = ClassifierParser(clf, vec, feature_fns)\n",
    "evaluate(parser, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [00:02<00:00, 4971.55it/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5954815095292187"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = compute_features(\n",
    "    generate_training_samples(train_data),\n",
    "    [stack_feature, stack_feature_2, input_feature, stack_pair_feature])\n",
    "vec = DictVectorizer()\n",
    "X_train = vec.fit_transform(dict(f) for f in features)\n",
    "X_train.shape\n",
    "clf = LogisticRegression(max_iter=100, class_weight='balanced')\n",
    "clf.fit(X_train, labels)\n",
    "parser = ClassifierParser(clf, vec, feature_fns)\n",
    "evaluate(parser, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [00:01<00:00, 5345.65it/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06550549034456646"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = compute_features(\n",
    "    generate_training_samples(train_data),\n",
    "    [stack_feature, input_feature_2, input_feature, stack_pair_feature])\n",
    "vec = DictVectorizer()\n",
    "X_train = vec.fit_transform(dict(f) for f in features)\n",
    "X_train.shape\n",
    "clf = LogisticRegression(max_iter=100, class_weight='balanced')\n",
    "clf.fit(X_train, labels)\n",
    "parser = ClassifierParser(clf, vec, feature_fns)\n",
    "evaluate(parser, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
